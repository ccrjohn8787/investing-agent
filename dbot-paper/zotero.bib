
@misc{zhang_multimodal_2024,
	title = {A {Multimodal} {Foundation} {Agent} for {Financial} {Trading}: {Tool}-{Augmented}, {Diversified}, and {Generalist}},
	shorttitle = {A {Multimodal} {Foundation} {Agent} for {Financial} {Trading}},
	url = {http://arxiv.org/abs/2402.18485},
	doi = {10.48550/arXiv.2402.18485},
	abstract = {Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with over 36\% average improvement on profit. Specifically, a 92.27\% return (a 84.39\% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Zhang, Wentao and Zhao, Lingxuan and Xia, Haochong and Sun, Shuo and Sun, Jiaze and Qin, Molei and Li, Xinyi and Zhao, Yuqing and Zhao, Yilei and Cai, Xinyu and Zheng, Longtao and Wang, Xinrun and An, Bo},
	month = jun,
	year = {2024},
	note = {arXiv:2402.18485 [q-fin]},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Finance - Trading and Market Microstructure},
}

@misc{han_enhancing_2024,
	title = {Enhancing {Investment} {Analysis}: {Optimizing} {AI}-{Agent} {Collaboration} in {Financial} {Research}},
	shorttitle = {Enhancing {Investment} {Analysis}},
	url = {http://arxiv.org/abs/2411.04788},
	doi = {10.48550/arXiv.2411.04788},
	abstract = {In recent years, the application of generative artificial intelligence (GenAI) in financial analysis and investment decision-making has gained significant attention. However, most existing approaches rely on single-agent systems, which fail to fully utilize the collaborative potential of multiple AI agents. In this paper, we propose a novel multi-agent collaboration system designed to enhance decision-making in financial investment research. The system incorporates agent groups with both configurable group sizes and collaboration structures to leverage the strengths of each agent group type. By utilizing a sub-optimal combination strategy, the system dynamically adapts to varying market conditions and investment scenarios, optimizing performance across different tasks. We focus on three sub-tasks: fundamentals, market sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30 companies listed on the Dow Jones Index. Our findings reveal significant performance variations based on the configurations of AI agents for different tasks. The results demonstrate that our multi-agent collaboration system outperforms traditional single-agent models, offering improved accuracy, efficiency, and adaptability in complex financial environments. This study highlights the potential of multi-agent systems in transforming financial analysis and investment decision-making by integrating diverse analytical perspectives.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04788 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Finance - Statistical Finance, Quantitative Finance - Trading and Market Microstructure},
}

@inproceedings{steiner_automate_2012,
	title = {Automate {This}: {How} {Algorithms} {Came} to {Rule} {Our} {World}},
	url = {https://api.semanticscholar.org/CorpusID:107220146},
	author = {Steiner, Christopher},
	year = {2012},
}

@misc{kwon_can_2024,
	title = {Can {GANs} {Learn} the {Stylized} {Facts} of {Financial} {Time} {Series}?},
	url = {http://arxiv.org/abs/2410.09850},
	doi = {10.48550/arXiv.2410.09850},
	abstract = {In the financial sector, a sophisticated financial time series simulator is essential for evaluating financial products and investment strategies. Traditional back-testing methods have mainly relied on historical data-driven approaches or mathematical model-driven approaches, such as various stochastic processes. However, in the current era of AI, data-driven approaches, where models learn the intrinsic characteristics of data directly, have emerged as promising techniques. Generative Adversarial Networks (GANs) have surfaced as promising generative models, capturing data distributions through adversarial learning. Financial time series, characterized 'stylized facts' such as random walks, mean-reverting patterns, unexpected jumps, and time-varying volatility, present significant challenges for deep neural networks to learn their intrinsic characteristics. This study examines the ability of GANs to learn diverse and complex temporal patterns (i.e., stylized facts) of both univariate and multivariate financial time series. Our extensive experiments revealed that GANs can capture various stylized facts of financial time series, but their performance varies significantly depending on the choice of generator architecture. This suggests that naively applying GANs might not effectively capture the intricate characteristics inherent in financial time series, highlighting the importance of carefully considering and validating the modeling choices.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Kwon, Sohyeon and Lee, Yongjae},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09850 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance},
}

@misc{kwon_can_2024-1,
	title = {Can {GANs} {Learn} the {Stylized} {Facts} of {Financial} {Time} {Series}?},
	url = {http://arxiv.org/abs/2410.09850},
	doi = {10.48550/arXiv.2410.09850},
	abstract = {In the financial sector, a sophisticated financial time series simulator is essential for evaluating financial products and investment strategies. Traditional back-testing methods have mainly relied on historical data-driven approaches or mathematical model-driven approaches, such as various stochastic processes. However, in the current era of AI, data-driven approaches, where models learn the intrinsic characteristics of data directly, have emerged as promising techniques. Generative Adversarial Networks (GANs) have surfaced as promising generative models, capturing data distributions through adversarial learning. Financial time series, characterized 'stylized facts' such as random walks, mean-reverting patterns, unexpected jumps, and time-varying volatility, present significant challenges for deep neural networks to learn their intrinsic characteristics. This study examines the ability of GANs to learn diverse and complex temporal patterns (i.e., stylized facts) of both univariate and multivariate financial time series. Our extensive experiments revealed that GANs can capture various stylized facts of financial time series, but their performance varies significantly depending on the choice of generator architecture. This suggests that naively applying GANs might not effectively capture the intricate characteristics inherent in financial time series, highlighting the importance of carefully considering and validating the modeling choices.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Kwon, Sohyeon and Lee, Yongjae},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09850 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance},
}

@inproceedings{tian_customized_2024,
	title = {Customized {FinGPT} {Search} {Agents} {Using} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2410.15284},
	doi = {10.1145/3677052.3698637},
	abstract = {Current large language models (LLMs) have proven useful for analyzing financial data, but most existing models, such as BloombergGPT and FinGPT, lack customization for specific user needs. In this paper, we address this gap by developing FinGPT Search Agents tailored for two types of users: individuals and institutions. For individuals, we leverage Retrieval-Augmented Generation (RAG) to integrate local documents and user-specified data sources. For institutions, we employ dynamic vector databases and fine-tune models on proprietary data. There are several key issues to address, including data privacy, the time-sensitive nature of financial information, and the need for fast responses. Experiments show that FinGPT agents outperform existing models in accuracy, relevance, and response time, making them practical for real-world applications.},
	urldate = {2025-01-21},
	booktitle = {Proceedings of the 5th {ACM} {International} {Conference} on {AI} in {Finance}},
	author = {Tian, Felix and Byadgi, Ajay and Kim, Daniel and Zha, Daochen and White, Matt and Xiao, Kairong and Yanglet, Xiao-Yang Liu},
	month = nov,
	year = {2024},
	note = {arXiv:2410.15284 [cs]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Human-Computer Interaction},
	pages = {469--477},
}

@misc{skarlinski_language_2024,
	title = {Language agents achieve superhuman synthesis of scientific knowledge},
	url = {http://arxiv.org/abs/2409.13740},
	doi = {10.48550/arXiv.2409.13740},
	abstract = {Language models are known to hallucinate incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of which 70\% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Skarlinski, Michael D. and Cox, Sam and Laurent, Jon M. and Braza, James D. and Hinks, Michaela and Hammerling, Michael J. and Ponnapati, Manvitha and Rodriques, Samuel G. and White, Andrew D.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.13740 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Physics - Physics and Society},
}

@misc{ziems_moral_2022,
	title = {The {Moral} {Integrity} {Corpus}: {A} {Benchmark} for {Ethical} {Dialogue} {Systems}},
	shorttitle = {The {Moral} {Integrity} {Corpus}},
	url = {http://arxiv.org/abs/2204.03021},
	abstract = {Content Warning: some examples in this paper may be offensive or upsetting.},
	language = {en},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Ziems, Caleb and Yu, Jane A. and Wang, Yi-Chia and Halevy, Alon and Yang, Diyi},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03021 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhu_tat-llm_2024,
	title = {{TAT}-{LLM}: {A} {Specialized} {Language} {Model} for {Discrete} {Reasoning} over {Tabular} and {Textual} {Data}},
	shorttitle = {{TAT}-{LLM}},
	url = {http://arxiv.org/abs/2401.13223},
	abstract = {In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-wise Pipeline. The experimental results have verified that our TAT-LLM model can outperform all baseline models, including the previous best fine-tuned models and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.},
	urldate = {2024-11-01},
	publisher = {arXiv},
	author = {Zhu, Fengbin and Liu, Ziyang and Feng, Fuli and Wang, Chao and Li, Moxin and Chua, Tat-Seng},
	month = sep,
	year = {2024},
	note = {arXiv:2401.13223},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhu_tat-llm_2024-1,
	title = {{TAT}-{LLM}: {A} {Specialized} {Language} {Model} for {Discrete} {Reasoning} over {Tabular} and {Textual} {Data}},
	shorttitle = {{TAT}-{LLM}},
	url = {http://arxiv.org/abs/2401.13223},
	abstract = {In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-wise Pipeline. The experimental results have verified that our TAT-LLM model can outperform all baseline models, including the previous best fine-tuned models and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.},
	urldate = {2024-11-01},
	publisher = {arXiv},
	author = {Zhu, Fengbin and Liu, Ziyang and Feng, Fuli and Wang, Chao and Li, Moxin and Chua, Tat-Seng},
	month = sep,
	year = {2024},
	note = {arXiv:2401.13223},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{fatemi_enhancing_2024,
	title = {Enhancing {Financial} {Question} {Answering} with a {Multi}-{Agent} {Reflection} {Framework}},
	url = {http://arxiv.org/abs/2410.21741},
	doi = {10.48550/arXiv.2410.21741},
	abstract = {While Large Language Models (LLMs) have shown impressive capabilities in numerous Natural Language Processing (NLP) tasks, they still struggle with financial question answering (QA), particularly when numerical reasoning is required. Recently, LLM-based multi-agent frameworks have demonstrated remarkable effectiveness in multi-step reasoning, which is crucial for financial QA tasks as it involves extracting relevant information from tables and text and then performing numerical reasoning on the extracted data to infer answers. In this study, we propose a multi-agent framework incorporating a critic agent that reflects on the reasoning steps and final answers for each question. Additionally, we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer. Our results indicate that this framework significantly improves performance compared to single-agent reasoning, with an average performance increase of 15\% for the LLaMA3-8B model and 5\% for the LLaMA3-70B model. Furthermore, our framework performs on par with, and in some cases surpasses, larger single-agent LLMs such as LLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to Claude-3.5 Sonnet. Overall, our framework presents an effective solution to enhance open-source LLMs for financial QA tasks, offering a cost-effective alternative to larger models like Claude-3.5 Sonnet.},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Fatemi, Sorouralsadat and Hu, Yuheng},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21741},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{fatouros_can_2024,
	title = {Can {Large} {Language} {Models} {Beat} {Wall} {Street}? {Unveiling} the {Potential} of {AI} in {Stock} {Selection}},
	shorttitle = {Can {Large} {Language} {Models} {Beat} {Wall} {Street}?},
	url = {http://arxiv.org/abs/2401.03737},
	doi = {10.48550/arXiv.2401.03737},
	abstract = {This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4's advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability and acceptance. Through empirical testing on the competitive S\&P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10\% to 30\% and achieving a cumulative return of up to 72\% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies.},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Fatouros, Georgios and Metaxas, Konstantinos and Soldatos, John and Kyriazis, Dimosthenis},
	month = apr,
	year = {2024},
	note = {arXiv:2401.03737 
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Quantitative Finance - Computational Finance},
}

@misc{fatouros_can_2024-1,
	title = {Can {Large} {Language} {Models} {Beat} {Wall} {Street}? {Unveiling} the {Potential} of {AI} in {Stock} {Selection}},
	shorttitle = {Can {Large} {Language} {Models} {Beat} {Wall} {Street}?},
	url = {http://arxiv.org/abs/2401.03737},
	doi = {10.48550/arXiv.2401.03737},
	abstract = {This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4's advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability and acceptance. Through empirical testing on the competitive S\&P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10\% to 30\% and achieving a cumulative return of up to 72\% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies.},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Fatouros, Georgios and Metaxas, Konstantinos and Soldatos, John and Kyriazis, Dimosthenis},
	month = apr,
	year = {2024},
	note = {arXiv:2401.03737 
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Quantitative Finance - Computational Finance},
}

@inproceedings{olausson_linc_2023,
	title = {{LINC}: {A} {Neurosymbolic} {Approach} for {Logical} {Reasoning} by {Combining} {Language} {Models} with {First}-{Order} {Logic} {Provers}},
	shorttitle = {{LINC}},
	url = {http://arxiv.org/abs/2310.15164},
	doi = {10.18653/v1/2023.emnlp-main.313},
	abstract = {Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38\% and 10\%, respectively. When used with GPT-4, LINC scores 26\% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available at https://github.com/benlipkin/linc},
	urldate = {2024-06-12},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Olausson, Theo X. and Gu, Alex and Lipkin, Benjamin and Zhang, Cedegao E. and Solar-Lezama, Armando and Tenenbaum, Joshua B. and Levy, Roger},
	year = {2023},
	note = {arXiv:2310.15164 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {5153--5176},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{lyu_faithful_2023,
	title = {Faithful {Chain}-of-{Thought} {Reasoning}},
	url = {http://arxiv.org/abs/2301.13379},
	doi = {10.48550/arXiv.2301.13379},
	abstract = {While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query \${\textbackslash}rightarrow\$ symbolic reasoning chain) and Problem Solving (reasoning chain \${\textbackslash}rightarrow\$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3\% on Math Word Problems (MWP), 3.4\% on Planning, 5.5\% on Multi-hop Question Answering (QA), and 21.4\% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
	month = sep,
	year = {2023},
	note = {arXiv:2301.13379 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{cai_low-code_2024,
	title = {Low-code {LLM}: {Graphical} {User} {Interface} over {Large} {Language} {Models}},
	shorttitle = {Low-code {LLM}},
	url = {http://arxiv.org/abs/2304.08103},
	doi = {10.48550/arXiv.2304.08103},
	abstract = {Utilizing Large Language Models (LLMs) for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper introduces a novel human-LLM interaction framework, Low-code LLM. It incorporates six types of simple low-code visual programming interactions to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the process without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow. We highlight three advantages of the low-code LLM: user-friendly interaction, controllable generation, and wide applicability. We demonstrate its benefits using four typical applications. By introducing this framework, we aim to bridge the gap between humans and LLMs, enabling more effective and efficient utilization of LLMs for complex tasks. The code, prompts, and experimental details are available at https://github.com/moymix/TaskMatrix/tree/main/LowCodeLLM. A system demonstration video can be found at https://www.youtube.com/watch?v=jb2C1vaeO3E.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Cai, Yuzhe and Mao, Shaoguang and Wu, Wenshan and Wang, Zehua and Liang, Yaobo and Ge, Tao and Wu, Chenfei and You, Wang and Song, Ting and Xia, Yan and Tien, Jonathan and Duan, Nan and Wei, Furu},
	month = apr,
	year = {2024},
	note = {arXiv:2304.08103 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{chen_open_2021,
	title = {Open {Question} {Answering} over {Tables} and {Text}},
	url = {http://arxiv.org/abs/2010.10439},
	doi = {10.48550/arXiv.2010.10439},
	abstract = {In open question answering (QA), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question. Most open QA systems have considered only retrieving information from unstructured text. Here we consider for the first time open QA over both tabular and textual data and present a new large-scale dataset Open Table-and-Text Question Answering (OTT-QA) to evaluate performance on this task. Most questions in OTT-QA require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging -- our baseline model using an iterative retriever and BERT-based reader achieves an exact match score less than 10\%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for OTT-QA. The first technique is to use "early fusion" to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27\%.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Chen, Wenhu and Chang, Ming-Wei and Schlinger, Eva and Wang, William and Cohen, William W.},
	month = feb,
	year = {2021},
	note = {arXiv:2010.10439 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chen_open_2021-1,
	title = {Open {Question} {Answering} over {Tables} and {Text}},
	url = {http://arxiv.org/abs/2010.10439},
	doi = {10.48550/arXiv.2010.10439},
	abstract = {In open question answering (QA), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question. Most open QA systems have considered only retrieving information from unstructured text. Here we consider for the first time open QA over both tabular and textual data and present a new large-scale dataset Open Table-and-Text Question Answering (OTT-QA) to evaluate performance on this task. Most questions in OTT-QA require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging -- our baseline model using an iterative retriever and BERT-based reader achieves an exact match score less than 10\%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for OTT-QA. The first technique is to use "early fusion" to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27\%.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Chen, Wenhu and Chang, Ming-Wei and Schlinger, Eva and Wang, William and Cohen, William W.},
	month = feb,
	year = {2021},
	note = {arXiv:2010.10439 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{wu_errudite_2019,
	address = {Florence, Italy},
	title = {Errudite: {Scalable}, {Reproducible}, and {Testable} {Error} {Analysis}},
	shorttitle = {Errudite},
	url = {https://aclanthology.org/P19-1073},
	doi = {10.18653/v1/P19-1073},
	abstract = {Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.},
	urldate = {2023-02-22},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel},
	month = jul,
	year = {2019},
	pages = {747--763},
}

@misc{komeili_internet-augmented_2021,
	title = {Internet-{Augmented} {Dialogue} {Generation}},
	url = {http://arxiv.org/abs/2107.07566},
	abstract = {The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020).},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
	month = jul,
	year = {2021},
	note = {arXiv:2107.07566 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_230803688_nodate,
	title = {[2308.03688] {AgentBench}: {Evaluating} {LLMs} as {Agents}},
	url = {https://arxiv.org/abs/2308.03688},
	urldate = {2023-09-26},
}
